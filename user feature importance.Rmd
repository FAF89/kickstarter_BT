---
title: "User Feature Importance"
author: "Ofir Mizrahi"
date: "2018 M05 24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
library(sqldf)
library(ggplot2)
```

##Load Data

We are loading only users with thier first project. Also, "personal" attribute is set as binary as results were better with it (instead of it being ordinal).

```{r}
path <- "C:\\Users\\Ofir\\Desktop\\Thesis\\Kickstarter\\userBinaryWeb.csv"
user <- read.csv(path, header = T)
user$X <- NULL

rm(path)
```


##Feature Importance for BIC
###Backward Step

Let's create a model with all features (backed + facebookFriends + continent + noFacebookFriends + personal) and then remove one of them and see how it affects the BIC measure.

We are using BIC meausure as there is not need to optimaize any parameter (such as threshold) as well as taking into conciderations the number of variables in the model.

The calculation done is part model BIC minus full model BIC. As we want to minimize BIC if we get:
a positive result, means adding the excluded feature to create the full model was creating a better model.
a negative result, means adding the excluded feature to create the full model was not needed and created a worst model.

We can see that "personal" was the most important feature which notes which users have a personal site. Such feature might show a user who is already consolidated with his project, has goals and maybe some products running. Shows seriousness.

Another highly important variable is the "backed" feature
Q: Due to the past plots and analysis done we raised a question of whether the influence on success is due to time on site (experience) or due to "hidden" variables of involvment on site such as "backed" and "comments" (which is not used here currently). Such result hints the answer is yes. As experience turned out not to be important while having backed in the model.

Continent is the least segnificant feature and only "hurts" the model. This is a good thing implying that the site is indeed globlized and success is not affected by location.


```{r Backward Step}
lmFull  <- glm(success ~ backed + facebookFriends + continent + noFacebookFriends + personal, family = binomial(), data = user)

#bic
fullBIC <- AIC(lmFull, k=log(nrow(user)))
# fullRAdj <- summary(lmFull)$adj.r.squared 

features <- names(user)[1:ncol(user)-1]
bic <- vector()
for (i in 1:length(features)) {
  current <- features[-i]
  string <- "success ~"
  for (j in 1:length(current)) {
    if(j==1){
      string <- paste(string,current[j])
    }else{
      string <- paste(string,"+",current[j])
    }
  }
  lmPart  <- glm(string, family = binomial(), data = user)
  partBIC <- AIC(lmPart, k=log(nrow(user)))
  bic[i] <- partBIC- fullBIC
}

res <- as.data.frame(cbind(features,bic))
res$bic <- as.numeric(as.character(res$bic))
ggplot(res)+geom_bar(aes(x=reorder(features, -bic), y=bic), stat = "identity")+xlab("Excluded Feature (from full model)")+ylab("BIC Difference (part-full)")+ggtitle("Step Backwards (BIC)")

rm(bic,res,fullBIC,lmFull,features,current,string,i,j,lmPart,partBIC)

```


###Forward Step

Let's now create a model with no features (intercept only) and then add only one of the features and see how it affects the BIC measure.

The calculation done is empty model BIC minus part model BIC. As we want to minimize BIC if we get:
a positive result, means adding the excluded feature to create the full model was creating a better model.
a negative result, means adding the excluded feature to create the full model was not needed and created a worst model.

In this plot we can again see the past discussed question about "backed" and "experience" and we can see that now when experience is the only variable in the model it has a major effect (second to "experience"backed). Experience holds within information that backed has.

Social network seems to be of an importance here- as oposed to the last plot- though only minor.


```{r Forward step}
lmEmpty <- glm(success ~ 1, family = binomial(), data = user)
emptyBIC <- AIC(lmEmpty, k=log(nrow(user)))


features <- names(user)[1:ncol(user)-1]
bic <- vector()
for (i in 1:length(features)) {
  current <- features[i]
  string <- paste("success ~",current)
  lmPart  <- glm(string, family = binomial(), data = user)
  partBIC <- AIC(lmPart, k=log(nrow(user)))
  bic[i] <- emptyBIC - partBIC 
}

res <- as.data.frame(cbind(features,bic))
res$bic <- as.numeric(as.character(res$bic))
ggplot(res)+geom_bar(aes(x=reorder(features, -bic), y=bic), stat = "identity")+xlab("Added Feature (to intercept)")+ylab("BIC Difference (empty-part)")+ggtitle("Step Forward (BIC)")

rm(bic,res,emptyBIC,lmEmpty,features,current,string,i,lmPart,partBIC)
```

##Feature Importance for Accuracy (prediction)

Accuracy can not be estimated by one run, therefore in order to find important features through accuracy differences we need to create a k-folds (k=10 in our case) run.
Such has been done for any result given from any model in this section (empty/part/full).


```{r K folds}
k <- 10
set.seed(01071989)
folds <- sample(k, nrow(user),replace=T)#sample random group number for each row

rm(k)
```

###Backward Step

Note: measurements like recall and percision had not been taken as is some runs results are so bad the model predicts only 1 category.

In the plot we can see the changes in the accuracy were really minor and I don't think very segnificant (of course this needs to be proven statisticaly). Also, a change in the threshold (now 0.5 by default) would change the feature importance as well. This is way I differ from taking such measure to select feature importance.

The biggest difference when comparing to the BIC feature selection is that here, "personal" is almost the worst feature for prediction.

```{r Backward step acc}
#run selected model on the 10 test sets to recieve an average of measures
accuracy <- c()
# recall <- c()
# precision <- c()


for (i in 1:10) {
  
  train   <- user[folds!=i,]
  test   <- user[folds==i,]
  
  lmFull  <- glm(success ~ backed + facebookFriends + continent + noFacebookFriends + personal, family = binomial(), data = train)
  
  #prediction from here
  lmPred   <- data.frame(test$success,responce=predict(lmFull, test, type='response'))
  
  lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
  
  accuracy[i] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
  
  conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
  
  # recall[i] <- conf_mat[2,2]/sum(conf_mat[,2])
  # precision[i] <- conf_mat[2,2]/sum(conf_mat[2,])
}

accFull <- mean(accuracy)
# recFull <- mean(recall)
# preFull <- mean(precision)


features <- names(user)[1:ncol(user)-1]
runsAcc <- vector()
# runsRec <- vector()
# runsPre <- vector()

for (i in 1:length(features)) {
  current <- features[-i]
  string <- "success ~"
  for (j in 1:length(current)) {
    if(j==1){
      string <- paste(string,current[j])
    }else{
      string <- paste(string,"+",current[j])
    }
  }
  
  accuracy <- vector()
  recall <- vector()
  precision <- vector()
  
  for (j in 1:10) {
    
    train   <- user[folds!=j,]
    test   <- user[folds==j,]
    
    lmPart  <- glm(string, family = binomial(), data = train)
    
    #prediction from here
    lmPred   <- data.frame(test$success,responce=predict(lmPart, test, type='response'))
    
    lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
    
    accuracy[j] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
    
    conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
    
    # recall[j] <- conf_mat[2,2]/sum(conf_mat[,2])
    # precision[j] <- conf_mat[2,2]/sum(conf_mat[2,])
  }
  
  runsAcc[i] <- accFull-mean(accuracy)
  # runsRec[i] <- recFull-mean(recall)
  # runsPre[i] <- preFull-mean(precision)
  
  
}

res <- as.data.frame(cbind(features,runsAcc))
res$runsAcc <- as.numeric(as.character(res$runsAcc))
ggplot(res)+geom_bar(aes(x=reorder(features, -runsAcc), y=runsAcc), stat = "identity")+xlab("Excluded Feature (from full model)")+ylab("Accuracy Difference (full-part)")+ggtitle("Step Backwards (ACC)")

# res <- as.data.frame(cbind(features,runsRec))
# res$runsRec <- as.numeric(as.character(res$runsRec))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsRec), y=runsRec), stat = "identity")
# 
# res <- as.data.frame(cbind(features,runsPre))
# res$runsPre <- as.numeric(as.character(res$runsPre))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsPre), y=runsPre), stat = "identity")

rm(lmFull,lmPart,lmPred,res,test,train,accFull,accuracy,conf_mat,current,features,i,j,precision,recall,runsAcc,string)

```

###Forward Step

Same idea as above.

```{r Forward step acc}
#run selected model on the 10 test sets to recieve an average of measures
accuracy <- c()
# recall <- c()
# precision <- c()

for (i in 1:10) {
  
  train   <- user[folds!=i,]
  test   <- user[folds==i,]
  
lmEmpty <- glm(success ~ 1, family = binomial(), data = user)  
  #prediction from here
  lmPred   <- data.frame(test$success,responce=predict(lmEmpty, test, type='response'))
  
  lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
  
  accuracy[i] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
  
  conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
  
  # recall[i] <- conf_mat[2,2]/sum(conf_mat[,2])
  # precision[i] <- conf_mat[2,2]/sum(conf_mat[2,])
}

accEmpty <- mean(accuracy)
# recEmpty <- mean(recall)
# preEmpty <- mean(precision)


features <- names(user)[1:ncol(user)-1]
runsAcc <- vector()
# runsRec <- vector()
# runsPre <- vector()

for (i in 1:length(features)) {
  current <- features[i]
  string <- paste("success ~",current)
  
  accuracy <- vector()
  # recall <- vector()
  # precision <- vector()
  
  for (j in 1:10) {
    
    train   <- user[folds!=j,]
    test   <- user[folds==j,]
    
    lmPart  <- glm(string, family = binomial(), data = train)
    
    #prediction from here
    lmPred   <- data.frame(test$success,responce=predict(lmPart, test, type='response'))
    
    lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
    
    accuracy[j] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
    
    conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
    
    # recall[j] <- conf_mat[2,2]/sum(conf_mat[,2])
    # precision[j] <- conf_mat[2,2]/sum(conf_mat[2,])
  }
  
  runsAcc[i] <- mean(accuracy)-accEmpty
  # runsRec[i] <- recEmpty-mean(recall)
  # runsPre[i] <- preEmpty-mean(precision)
  
  
}

res <- as.data.frame(cbind(features,runsAcc))
res$runsAcc <- as.numeric(as.character(res$runsAcc))
ggplot(res)+geom_bar(aes(x=reorder(features, -runsAcc), y=runsAcc), stat = "identity")+xlab("Added Feature (to intercept)")+ylab("Accuracy Difference (part-empty)")+ggtitle("Step Forward (ACC)")

# res <- as.data.frame(cbind(features,runsRec))
# res$runsRec <- as.numeric(as.character(res$runsRec))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsRec), y=runsRec), stat = "identity")
# 
# res <- as.data.frame(cbind(features,runsPre))
# res$runsPre <- as.numeric(as.character(res$runsPre))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsPre), y=runsPre), stat = "identity")

rm(lmEmpty,lmPart,lmPred,res,test,train,accEmpty,accuracy,conf_mat,current,features,i,j,runsAcc,string)
```

