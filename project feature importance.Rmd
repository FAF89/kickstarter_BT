---
title: "Project Feature Importance"
author: "Ofir Mizrahi"
date: "2018 M05 24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F)
library(sqldf)
library(ggplot2)
```

##Load Data

We are loading only projects with thier first project. Also, "personal" attribute is set as binary as results were better with it (instead of it being ordinal).

```{r}
path <- "C:\\Users\\Ofir\\Desktop\\Thesis\\Kickstarter\\projectBinaryWeb.csv"
project <- read.csv(path, header = T)
project$X <- NULL

project <- project[,c(4:10,17)]
rm(path)
```


##Feature Importance for BIC
###Backward Step

Let's create a model with all features and then remove one of them and see how it affects the BIC measure.

We are using BIC meausure as there is not need to optimaize any parameter (such as threshold) as well as taking into conciderations the number of variables in the model.

The calculation done is part model BIC minus full model BIC. As we want to minimize BIC if we get:
a positive result, means adding the excluded feature to create the full model was creating a better model.
a negative result, means adding the excluded feature to create the full model was not needed and created a worst model.

The commentary on the plot is pretty stright forward.

Q:We will note that "numUpdates" is unbelievably high in all plots which makes me suspect something is wrong here, or- maybe it is very important. We'll have to look into that.

The sites were not very segnificant here though this might be due to the lack of value in most projects for them.

```{r Backward Step}

features <- names(project)[1:ncol(project)-1]

string <- "success ~"
for (j in 1:length(features)) {
  if(j==1){
    string <- paste(string,features[j])
  }else{
    string <- paste(string,"+",features[j])
  }
}

lmFull  <- glm(formula= string , family = binomial(), data = project)


#bic
fullBIC <- AIC(lmFull, k=log(nrow(project)))

bic <- vector()
for (i in 1:length(features)) {
  current <- features[-i]
  string <- "success ~"
  for (j in 1:length(current)) {
    if(j==1){
      string <- paste(string,current[j])
    }else{
      string <- paste(string,"+",current[j])
    }
  }
  lmPart  <- glm(string, family = binomial(), data = project)
  partBIC <- AIC(lmPart, k=log(nrow(project)))
  bic[i] <- partBIC- fullBIC
}

features <- c("num. words", "main video", "num. additional videos", "num. images", "view gallery", "estimated delivery time", "shipping spread" )
res <- as.data.frame(cbind(features,bic))
res$bic <- as.numeric(as.character(res$bic))
ggplot(res)+geom_bar(aes(x=reorder(features, -bic), y=bic, fill="blue"), stat = "identity")+xlab("Excluded Feature (from full model)")+ylab("BIC Difference (part-full)")+ggtitle("Step Backwards (BIC)")+ theme(axis.text.x = element_text(angle = 45, hjust = 1),text = element_text(size=20),title =element_text(colour="black",face="bold"))+scale_fill_manual(values = c("#3690c0"))


#rm(bic,res,fullBIC,lmFull,features,current,string,i,j,lmPart,partBIC)

```


###Forward Step

Let's now create a model with no features (intercept only) and then add only one of the features and see how it affects the BIC measure.

The calculation done is empty model BIC minus part model BIC. As we want to minimize BIC if we get:
a positive result, means adding the excluded feature to create the full model was creating a better model.
a negative result, means adding the excluded feature to create the full model was not needed and created a worst model.

Steps forward is always somwhat more revealing. Here we can see that features such as "numImages" and "totWordCount" that were rankes very low in the steps backwards plot are now ranked very high.


```{r Forward step}
lmEmpty <- glm(success ~ 1, family = binomial(), data = project)
emptyBIC <- AIC(lmEmpty, k=log(nrow(project)))


features <- names(project)[1:ncol(project)-1]
bic <- vector()
for (i in 1:length(features)) {
  current <- features[i]
  string <- paste("success ~",current)
  lmPart  <- glm(string, family = binomial(), data = project)
  partBIC <- AIC(lmPart, k=log(nrow(project)))
  bic[i] <- emptyBIC - partBIC 
}

features <- c("num. words", "main video", "num. additional videos", "num. images", "view gallery", "estimated delivery time", "shipping spread" )
res <- as.data.frame(cbind(features,bic))
res$bic <- as.numeric(as.character(res$bic))
ggplot(res)+geom_bar(aes(x=reorder(features, -bic), y=bic,fill="blue"), stat = "identity")+xlab("Added Feature (to intercept)")+ylab("BIC Difference (empty-part)")+ggtitle("Step Forward (BIC)")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),text = element_text(size=20),title =element_text(colour="black",face="bold"))+scale_fill_manual(values = c("#ec7014"))


#rm(bic,res,emptyBIC,lmEmpty,features,current,string,i,lmPart,partBIC)
```

##Feature Importance for Accuracy (prediction)

Accuracy can not be estimated by one run, therefore in order to find important features through accuracy differences we need to create a k-folds (k=10 in our case) run.
Such has been done for any result given from any model in this section (empty/part/full).


```{r K folds}
k <- 10
set.seed(01071989)
folds <- sample(k, nrow(project),replace=T)#sample random group number for each row
```

###Backward Step

Note: measurements like recall and percision had not been taken as is some runs results are so bad the model predicts only 1 category.

In the plot we can see the changes in the accuracy were really minor and I don't think very segnificant (of course this needs to be proven statisticaly). Also, a change in the threshold (now 0.5 by default) would change the feature importance as well. This is way I differ from taking such measure to select feature importance.

The biggest difference when comparing to the BIC feature selection is that here, "personal" is almost the worst feature for prediction.

```{r Backward step acc}
#run selected model on the 10 test sets to recieve an average of measures
accuracy <- c()
# recall <- c()
# precision <- c()


features <- names(project)[1:ncol(project)-1]

string <- "success ~"
for (j in 1:length(features)) {
  if(j==1){
    string <- paste(string,features[j])
  }else{
    string <- paste(string,"+",features[j])
  }
}

for (i in 1:k) {
  
  train   <- project[folds!=i,]
  test   <- project[folds==i,]
  
  lmFull  <- glm(formula= string, family = binomial(), data = train)
  
  #prediction from here
  lmPred   <- data.frame(test$success,responce=predict(lmFull, test, type='response'))
  
  lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
  
  accuracy[i] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
  
  conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
  
  # recall[i] <- conf_mat[2,2]/sum(conf_mat[,2])
  # precision[i] <- conf_mat[2,2]/sum(conf_mat[2,])
}

accFull <- mean(accuracy)
# recFull <- mean(recall)
# preFull <- mean(precision)


features <- names(project)[1:ncol(project)-1]
runsAcc <- vector()
# runsRec <- vector()
# runsPre <- vector()

for (i in 1:length(features)) {
  current <- features[-i]
  string <- "success ~"
  for (j in 1:length(current)) {
    if(j==1){
      string <- paste(string,current[j])
    }else{
      string <- paste(string,"+",current[j])
    }
  }
  
  accuracy <- vector()
  recall <- vector()
  precision <- vector()
  
  for (j in 1:10) {
    
    train   <- project[folds!=j,]
    test   <- project[folds==j,]
    
    lmPart  <- glm(string, family = binomial(), data = train)
    
    #prediction from here
    lmPred   <- data.frame(test$success,responce=predict(lmPart, test, type='response'))
    
    lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
    
    accuracy[j] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
    
    conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
    
    # recall[j] <- conf_mat[2,2]/sum(conf_mat[,2])
    # precision[j] <- conf_mat[2,2]/sum(conf_mat[2,])
  }
  
  runsAcc[i] <- accFull-mean(accuracy)
  # runsRec[i] <- recFull-mean(recall)
  # runsPre[i] <- preFull-mean(precision)
  
  
}

res <- as.data.frame(cbind(features,runsAcc))
res$runsAcc <- as.numeric(as.character(res$runsAcc))
ggplot(res)+geom_bar(aes(x=reorder(features, -runsAcc), y=runsAcc), stat = "identity")+xlab("Excluded Feature (from full model)")+ylab("Accuracy Difference (full-part)")+ggtitle("Step Backwards (ACC)")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))

# res <- as.data.frame(cbind(features,runsRec))
# res$runsRec <- as.numeric(as.character(res$runsRec))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsRec), y=runsRec), stat = "identity")
# 
# res <- as.data.frame(cbind(features,runsPre))
# res$runsPre <- as.numeric(as.character(res$runsPre))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsPre), y=runsPre), stat = "identity")

rm(lmFull,lmPart,lmPred,res,test,train,accFull,accuracy,conf_mat,current,features,i,j,precision,recall,runsAcc,string)

```

###Forward Step

Same idea as above.

"personal" somewhat acts weird just as in the user feature importace plots. It's ranked high in the BIC case and lowest in the ACC case.

```{r Forward step acc}
#run selected model on the 10 test sets to recieve an average of measures
accuracy <- c()
# recall <- c()
# precision <- c()

for (i in 1:k) {
  
  train   <- project[folds!=i,]
  test   <- project[folds==i,]
  
  lmEmpty <- glm(success ~ 1, family = binomial(), data = project)  
  #prediction from here
  lmPred   <- data.frame(test$success,responce=predict(lmEmpty, test, type='response'))
  
  lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
  
  accuracy[i] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
  
  conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
  
  # recall[i] <- conf_mat[2,2]/sum(conf_mat[,2])
  # precision[i] <- conf_mat[2,2]/sum(conf_mat[2,])
}

accEmpty <- mean(accuracy)
# recEmpty <- mean(recall)
# preEmpty <- mean(precision)


features <- names(project)[1:ncol(project)-1]
runsAcc <- vector()
# runsRec <- vector()
# runsPre <- vector()

for (i in 1:length(features)) {
  current <- features[i]
  string <- paste("success ~",current)
  
  accuracy <- vector()
  # recall <- vector()
  # precision <- vector()
  
  for (j in 1:10) {
    
    train   <- project[folds!=j,]
    test   <- project[folds==j,]
    
    lmPart  <- glm(string, family = binomial(), data = train)
    
    #prediction from here
    lmPred   <- data.frame(test$success,responce=predict(lmPart, test, type='response'))
    
    lmPred$prediction <- ifelse(lmPred$responce > 0.5,1,0)
    
    accuracy[j] <- sum(lmPred$test.success==lmPred$prediction)/dim(lmPred)[1]
    
    conf_mat  <- table(predicted = lmPred$prediction, actual = lmPred$test.success)
    
    # recall[j] <- conf_mat[2,2]/sum(conf_mat[,2])
    # precision[j] <- conf_mat[2,2]/sum(conf_mat[2,])
  }
  
  runsAcc[i] <- mean(accuracy)-accEmpty
  # runsRec[i] <- recEmpty-mean(recall)
  # runsPre[i] <- preEmpty-mean(precision)
  
  
}

res <- as.data.frame(cbind(features,runsAcc))
res$runsAcc <- as.numeric(as.character(res$runsAcc))
ggplot(res)+geom_bar(aes(x=reorder(features, -runsAcc), y=runsAcc), stat = "identity")+xlab("Added Feature (to intercept)")+ylab("Accuracy Difference (part-empty)")+ggtitle("Step Forward (ACC)")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))

# res <- as.data.frame(cbind(features,runsRec))
# res$runsRec <- as.numeric(as.character(res$runsRec))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsRec), y=runsRec), stat = "identity")
# 
# res <- as.data.frame(cbind(features,runsPre))
# res$runsPre <- as.numeric(as.character(res$runsPre))
# ggplot(res)+geom_bar(aes(x=reorder(features, -runsPre), y=runsPre), stat = "identity")

rm(lmEmpty,lmPart,lmPred,res,test,train,accEmpty,accuracy,conf_mat,current,features,i,j,runsAcc,string)
```

